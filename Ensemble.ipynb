{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align = \"center\"> Ensemble</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some important terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Bagging, Random Forest, Boosting\n",
    "* For bagging, we end up with having 63% of unique observations\n",
    "* To measure the confidence of the classification, we can either use margin (the difference between the majority vote and the minority vote\n",
    "* In a real-world situation, we generally want to have a much larger test set to estimate our unseen accuracy. In fact, because of this, we are more inclined to believe our outof-bag accuracy measurement, which is done over a larger number of observations and averaged over many models.\n",
    "* Adaboost is basically update the coefficient weight for each weak learner, update the weights for each data points all based on the weighted classification error and the final model is the sign of the sum of all the weak learners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Strenghts     | Weakness\n",
    "  ------------- | -------------\n",
    "  1) Smoothens the overall output and reduce the bias when the target function is smooth| Sampling<br/> 1) sampling might have ignored the level in the categorical variables with very few observations <br/> 2) Unbalanced sample will always favor the majority one <br/>3) High Leverage points bagging performance depends on how often these particular observations are sampled <br/> Bagging: <br/>4) Not truly independent (the same set of the features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(ipred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baggedTree = bagging(y~., data = , nbagg = 100, coob = T) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "setwd(\"E:/Personal/InterviewQuestion/Rscripts\")\n",
    "bnote = read.table(\"data_banknote_authentication.txt\", sep = \",\")\n",
    "colnames(bnote) = c(\"waveletVar\", \"waveletSkew\", \"waveletCurt\", \"entropy\",\"class\")\n",
    "#bnote$class = as.factor(bnote$class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>waveletVar</th><th scope=col>waveletSkew</th><th scope=col>waveletCurt</th><th scope=col>entropy</th><th scope=col>class</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>3.6216  </td><td>8.6661  </td><td>-2.8073 </td><td>-0.44699</td><td>1       </td></tr>\n",
       "\t<tr><th scope=row>2</th><td>4.5459 </td><td>8.1674 </td><td>-2.4586</td><td>-1.4621</td><td>1      </td></tr>\n",
       "\t<tr><th scope=row>3</th><td>3.866  </td><td>-2.6383</td><td>1.9242 </td><td>0.10645</td><td>1      </td></tr>\n",
       "\t<tr><th scope=row>4</th><td>3.4566 </td><td>9.5228 </td><td>-4.0112</td><td>-3.5944</td><td>1      </td></tr>\n",
       "\t<tr><th scope=row>5</th><td>0.32924</td><td>-4.4552</td><td>4.5718 </td><td>-0.9888</td><td>1      </td></tr>\n",
       "\t<tr><th scope=row>6</th><td>4.3684 </td><td>9.6718 </td><td>-3.9606</td><td>-3.1625</td><td>1      </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllll}\n",
       "  & waveletVar & waveletSkew & waveletCurt & entropy & class\\\\\n",
       "\\hline\n",
       "\t1 & 3.6216   & 8.6661   & -2.8073  & -0.44699 & 1       \\\\\n",
       "\t2 & 4.5459  & 8.1674  & -2.4586 & -1.4621 & 1      \\\\\n",
       "\t3 & 3.866   & -2.6383 & 1.9242  & 0.10645 & 1      \\\\\n",
       "\t4 & 3.4566  & 9.5228  & -4.0112 & -3.5944 & 1      \\\\\n",
       "\t5 & 0.32924 & -4.4552 & 4.5718  & -0.9888 & 1      \\\\\n",
       "\t6 & 4.3684  & 9.6718  & -3.9606 & -3.1625 & 1      \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "  waveletVar waveletSkew waveletCurt  entropy class\n",
       "1    3.62160      8.6661     -2.8073 -0.44699     0\n",
       "2    4.54590      8.1674     -2.4586 -1.46210     0\n",
       "3    3.86600     -2.6383      1.9242  0.10645     0\n",
       "4    3.45660      9.5228     -4.0112 -3.59440     0\n",
       "5    0.32924     -4.4552      4.5718 -0.98880     0\n",
       "6    4.36840      9.6718     -3.9606 -3.16250     0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(bnote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = dim(bnote)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the bootstrapped samples by sampling the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M = 11\n",
    "seeds = 1000: (1000+M-1)\n",
    "sample_vector = sapply(seeds, function(i){\n",
    "    set.seed(i); return (sample(n,n,replace = T))})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create one initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_1glm = function(sample_indices){\n",
    "    data = bnote[sample_indices,]\n",
    "    model = glm(class~., data = data, family = binomial(\"logit\"))\n",
    "    return(model)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      ": glm.fit: fitted probabilities numerically 0 or 1 occurredWarning message:\n",
      ": glm.fit: fitted probabilities numerically 0 or 1 occurredWarning message:\n",
      ": glm.fit: fitted probabilities numerically 0 or 1 occurredWarning message:\n",
      ": glm.fit: fitted probabilities numerically 0 or 1 occurredWarning message:\n",
      ": glm.fit: fitted probabilities numerically 0 or 1 occurredWarning message:\n",
      ": glm.fit: fitted probabilities numerically 0 or 1 occurredWarning message:\n",
      ": glm.fit: fitted probabilities numerically 0 or 1 occurredWarning message:\n",
      ": glm.fit: fitted probabilities numerically 0 or 1 occurredWarning message:\n",
      ": glm.fit: fitted probabilities numerically 0 or 1 occurredWarning message:\n",
      ": glm.fit: fitted probabilities numerically 0 or 1 occurredWarning message:\n",
      ": glm.fit: fitted probabilities numerically 0 or 1 occurred"
     ]
    }
   ],
   "source": [
    "allModels = apply(sample_vector,2,train_1glm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find unique observations in each data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " get_1bag <- function(sample_indices) {     \n",
    "     unique_sample <- unique(sample_indices)\n",
    "     df <- heart_train[unique_sample, ]\n",
    "     df$ID <- unique_sample\n",
    "     return(df)  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bags = apply(sample_vectors, 2, get_1bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "1372"
      ],
      "text/latex": [
       "1372"
      ],
      "text/markdown": [
       "1372"
      ],
      "text/plain": [
       "[1] 1372"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "glm_prediction = function(model, data, model_index){\n",
    "    colname = paste(\"prediction\", model_index)\n",
    "    data[colname] = as.numeric(predict(model, data, type = \"response\") >0.5)\n",
    "    return(data[,c(\"ID\", colname)], drop = FALSE)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_predictions =  mapply(glm_predictions, models, bags, 1 : M, SIMPLIFY = F) \n",
    "# mapply is to  applies FUN to the first elements of each ... argument, the second elements, the third elements, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rain_pred_df <- Reduce(function(x, y) merge(x, y, by = \"ID\", all = T), training_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pred_vote <- apply(train_pred_df[,-1], 1, function(x) as.numeric(mean(x, na.rm = TRUE) > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Then merge all the data and its prediction to one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_1oo_bag <- function(sample_indices) {  unique_sample <- setdiff(1 : n, unique(sample_indices))\n",
    "                                         df <- bnote[unique_sample,]     \n",
    "                                         df$ID <- unique_sample\n",
    "                                         return(df)  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "oo_bags <- apply(sample_vector, 2, get_1oo_bag) # Each list has different size of observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Strenghts     | Weakness\n",
    "  ------------- | -------------\n",
    "  1) combining the weak learners can help reduce overfitting|  1) Use all training data and progressively to correct each mistake <br/> 2) Symetric loss funtction no distinction that is made in classification between a fp and fn error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AdaBoostNN <- function(training_data, output_column, M, hidden_units) {   \n",
    "    require(\"nnet\")   \n",
    "    models <- list()   \n",
    "    alphas <- list()   \n",
    "    n <- nrow(training_data)   \n",
    "    model_formula <- as.formula(paste(output_column, '~ .', sep = ''))   \n",
    "    w <- rep((1/n), n)   \n",
    "    for (m in 1:M) {    model <- nnet(model_formula, data = training_data, size = hidden_units, weights = w)   \n",
    "                        models[[m]] <- model    \n",
    "                        predictions <- as.numeric(predict(model,\n",
    "                                                          training_data[, -which(names(training_data) == output_column)], type = \"class\")) \n",
    "                        errors <- predictions != training_data[, output_column]     \n",
    "                        error_rate <- sum(w * as.numeric(errors)) / sum(w)    \n",
    "                         alpha <- 0.5 * log((1 - error_rate) / error_rate)     \n",
    "                         alphas[[m]] <- alpha     \n",
    "                         temp_w <- mapply(function(x, y) if (y) { x * exp(alpha) } else { x * exp(-alpha)}, w, errors)     \n",
    "                                 w <- temp_w / sum(temp_w)   }   \n",
    "                        return(list(models = models, alphas = unlist(alphas))) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Stochastic Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "in every iteration of boosting, we compute a gradient in the direction of the errors that are made by the model trained in the current iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N <- 1000\n",
    "X1 <- runif(N)\n",
    "X2 <- 2*runif(N)\n",
    "X3 <- ordered(sample(letters[1:4],N,replace=TRUE),levels=letters[4:1])\n",
    "X4 <- factor(sample(letters[1:6],N,replace=TRUE))\n",
    "X5 <- factor(sample(letters[1:3],N,replace=TRUE))\n",
    "X6 <- 3*runif(N) \n",
    "mu <- c(-1,0,1,2)[as.numeric(X3)]\n",
    "\n",
    "SNR <- 10 # signal-to-noise ratio\n",
    "Y <- X1**1.5 + 2 * (X2**.5) + mu\n",
    "sigma <- sqrt(var(Y)/SNR)\n",
    "Y <- Y + rnorm(N,0,sigma)\n",
    "\n",
    "# introduce some missing values\n",
    "X1[sample(1:N,size=500)] <- NA\n",
    "X4[sample(1:N,size=300)] <- NA\n",
    "\n",
    "data <- data.frame(Y=Y,X1=X1,X2=X2,X3=X3,X4=X4,X5=X5,X6=X6)\n",
    "\n",
    "# fit initial model\n",
    "gbm1 <-\n",
    "gbm(Y~X1+X2+X3+X4+X5+X6,         # formula\n",
    "    data=data,                   # dataset\n",
    "    var.monotone=c(0,0,0,0,0,0), # -1: monotone decrease,\n",
    "                                 # +1: monotone increase,\n",
    "                                 #  0: no monotone restrictions\n",
    "    distribution=\"gaussian\",     # For regression model\n",
    "    n.trees=1000,                # number of trees\n",
    "    shrinkage=0.05,              # shrinkage or learning rate,(how fast the algorithm is going to learn the parameter before each sequential model)\n",
    "                                 # 0.001 to 0.1 usually work\n",
    "    interaction.depth=3,         # 1: additive model, 2: two-way interactions, etc.\n",
    "    bag.fraction = 0.5,          # subsampling fraction, 0.5 is probably best\n",
    "    train.fraction = 0.5,        # fraction of data for training,\n",
    "                                 # first train.fraction*N used for training\n",
    "    n.minobsinnode = 10,         # minimum total weight needed in each node\n",
    "    cv.folds = 3,                # do 3-fold cross-validation\n",
    "    keep.data=TRUE,              # keep a copy of the dataset with the object\n",
    "    verbose=FALSE,               # don't print out progress\n",
    "    n.cores=1)                   # use only a single core (detecting #cores is\n",
    "                                 # error-prone, so avoided here)\n",
    "\n",
    "# check performance using an out-of-bag estimator\n",
    "# OOB underestimates the optimal number of iterations\n",
    "best.iter <- gbm.perf(gbm1,method=\"OOB\") # Get the best iteration\n",
    "print(best.iter)\n",
    "\n",
    "# check performance using a 50% heldout test set\n",
    "best.iter <- gbm.perf(gbm1,method=\"test\")\n",
    "print(best.iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest (see the conversion example in take home) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Strenghts     | Weakness\n",
    "  ------------- | -------------\n",
    "  1) Feature sampling allows us to consider input features that are successful in splitting the data for only a small range of the target variable. These locally relevant features are rarely chosen without the sampling constraint because we usually prefer features that form good overall splits of the data at a given node in the tree. <br/> 2)sampling input features is useful when we have correlated input features. Regular tree construction tends to favor only one of the features from a correlated set while ignoring the rest despite the fact that the resulting splits from even highly correlated features are not exactly the same.<br/> 3) , this model is a good choice when the number of features exceeds the number of observations. <br/> 4) the sampling process mitigates the cost of constructing a large number of trees </br> 5) Can handle noisy or missing outliers data as well as categorical, continuous features </br> 6) did feature selection </br>|  1)  Unlike a decision tree, the model is not easily interpretable 2)  May require some work to tune the model to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "randomForest 4.6-12\n",
      "Type rfNews() to see new features/changes/bug fixes.\n"
     ]
    }
   ],
   "source": [
    "library(randomForest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(e1071)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_ranges <- list(ntree = c(500, 1000, 1500, 2000), mtry = 3:8) # Very common to build somewhere between 700 and several thousand trees\n",
    "#n rees is the number of trees that will be built for the ensemble and mtry is the number of features sampled for use at each node for spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_tune <- tune(randomForest, LeagueIndex ~ ., data =  skillcraft_train, ranges = rf_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_tune$best.parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For random forests, it turns out that we can still measure variable importance scores for the different input features by tallying and keeping track of the reductions in our error function across all the trees in the ensemble. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Use importance function to find the most important one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "randomForest(x, y=NULL, xtest=NULL, ytest=NULL, \n",
    "             ntree=500,#How many trees\n",
    "             mtry=, #How many features for each split\n",
    "             replace=TRUE, \n",
    "             classwt=NULL, \n",
    "             \n",
    "             \n",
    "#Priors of the classes. Need not add up to one. \n",
    " #The basic idea is to weight classes such that rarely observations are more likely to be selected to the group\n",
    " #these values are trasformed in probabilities for sampling training data\n",
    "             cutoff, \n",
    "             strata,\n",
    "             sampsize = if (replace) nrow(x) else ceiling(.632*nrow(x)),\n",
    "             nodesize = if (!is.null(y) && !is.factor(y)) 5 else 1,\n",
    "             maxnodes = NULL,\n",
    "             importance=FALSE, \n",
    "#. MeanDecreaseAccuracy is computed from permuting OOB data: For each tree, the prediction error on the out-of-bag portion of the data is\n",
    "# recorded (error rate for classification, MSE for regression). Then the same is done after permuting\n",
    "# each predictor variable. The difference between the two are then averaged over all trees, and normalized\n",
    "# by the standard deviation of the differences. If the standard deviation of the differences is\n",
    "# equal to 0 for a variable, the division is not done (but the average is almost always equal to 0 in that\n",
    "    # case).\n",
    "# MeanDecreaseGini is the total decrease in node impurities from splitting on the variable, averaged over all trees. \n",
    "# For classification, the node impurity is measured by the Gini index. For regression, it\n",
    "                    # is measured by residual sum of squares.\n",
    "  \n",
    "                 localImp=FALSE, nPerm=1,\n",
    "             proximity, #Should proximity measure among the rows be calculated? \n",
    " #It is like clustering as we assume the observations which fall into the same terminal node are more similar \n",
    "#Since observations that have similar x values ‘travel’ the same way on splits more often than values\n",
    "# with dissimilar values, the co-occurence in terminal nodes is a suitable measure of similarity. Using this logic\n",
    "# a so called proximity matrix is calculated. \n",
    "# It is a n by n matrix where each entry gives the proportion of terminal nodes that two observations are fall together\n",
    "             oob.prox=proximity,\n",
    "             norm.votes=TRUE, \n",
    "             do.trace=FALSE,\n",
    "             keep.forest=,\n",
    "             corr.bias=FALSE,\n",
    "             keep.inbag=FALSE, ...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
