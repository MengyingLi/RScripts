{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align = \"center\"> Regularization </h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why use regularization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://people.ee.duke.edu/~lcarin/Minhua11.7.08.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization Method| Form | Pros| Cons|\n",
    "------------|----------|-------|------|\n",
    "L1: Lasso| absolute value of Beta_i| Automation of predictor selection| 1) it will not necessarily yield good results in presence of high collinearity (it has been observed that if predictors are highly correlated, the prediction performance of the lasso is dominated by ridge regression). <br/> 2) The second problem with L1 penalty is that the lasso solution is not uniquely determined when the number of variables is greater than the number of subjects (this is not the case of ridge regression).<br/> 3) The last drawback of lasso is that it tends to select only one variable among a group of predictors with high pairwise correlations. So probably one group should be selected as a whole when doing the prediction, only include one level does not make sense <br/> 4) It can only select at most n variables. |\n",
    "L2: Ridge| sum of squared of the coeffiencts| Perform better than OLS,through a better compromise between bias and variance.| Its main drawback is that all predictors are kept in the model, so it is not very interesting if you seek a parsimonious model or want to apply some kind of feature selection| \n",
    "Elastic Net|lamba( (1-alpha)*lasso + alpha* ridge)| Combination of Lasso and Ridge but include both/can do group (highly correlated predictors will have similar regression coefficients.)/Strictly Convex has unique solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: Matrix\n",
      "Loading required package: foreach\n",
      "Loaded glmnet 2.0-5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(glmnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two Important Plot\n",
    "\n",
    "1) lambda or L1 norm against the coefficient values. The value will become zero when lambda is large or the L1 norm is small\n",
    "2) To choose the lambda: use cv.glmnet (cross validation) plot either mean-squared error or misclassification rate against log(lambda) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* alpha is for the elastic-net mixing parameter αα, with range α∈[0,1]α∈[0,1]. α=1α=1 is the lasso (default) and α=0α=0 is the ridge.\n",
    "\n",
    "* use the cv.glmnet for different alpha and choose the lambda.min which gives the least error and then compare the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
