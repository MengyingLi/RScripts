{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align = \"center\"> SVM </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some important terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hyperplane: a flat boundary to create fairly homogeneous partitions on either side. Combination of KNN and Linear regression.\n",
    "* Maximum margin hyperplane: creates the greatest separation between the two classes. The line that leads to the greatest separation will generalize the best of the future data. The reason is that in the future, the new data point might be very close to the boundary and the margin-maximizing boundary gives the maximal leeway for classifying such points and if you want to misclassify any points you need to place it further into the margin than any other linear discriminant.\n",
    "* The loss function is a hinge loss function: It won't penalize a point until it goes beyond the wrong side's margin (linear to the distance to the wrong side margin). If it is still within the margin, it did not get penalized.\n",
    "* Support Vectors: points from each class that are the closest to the MMH. It can define the MMH\n",
    "* Convex Hull: Outer boundary of these two groups of data points. MMH is going to be the perpendicular bisector of the shortest line between the two convex hulls. Quadratic optimization are capable of finding the maximum margin. \n",
    "* Cost value: The larger the cost value, the harder the optimization will try to achieve 100 percent separation. A lower cost parameter will place the emphasis on a wider overall margin\n",
    "* Kernel: dot product of the feature vectors (Gaussion RBF kernel a reasonable starting point for many learning tasks. Map the original feature to a new feature space and fit the model in the new feature space. Higher order combinations of these features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> <font color = \"red\"> Maximize the margin + minimize the training error </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f = wx +b (kernel trick just change the w to the sum of some transformation of original xi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Strenghts     | Weakness\n",
    "  ------------- | -------------\n",
    "  1) Can be used for classification or numeric prediction </br> 2) Not overly influenced by noisy data and not very prone to overfitting <br/>3) high accuracy| 1) Finding the best model requires testing of various combinations of kernels and model parameters <br/>2) Can be slow to train, particularly if the input dataset has a large number of features or examples<br/> 3)Hard to interpret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Parameters to think"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Cost parameter\n",
    "2. Kernel\n",
    "3. Cross validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "letters = read.table(\"https://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data\", sep = \",\", header = TRUE,as.is = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>T</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>I</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>D</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>N</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>G</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>S</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>B</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|l}\n",
       "  & T\\\\\n",
       "\\hline\n",
       "\t1 & I\\\\\n",
       "\t2 & D\\\\\n",
       "\t3 & N\\\\\n",
       "\t4 & G\\\\\n",
       "\t5 & S\\\\\n",
       "\t6 & B\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "  T\n",
       "1 I\n",
       "2 D\n",
       "3 N\n",
       "4 G\n",
       "5 S\n",
       "6 B"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(letters[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t19999 obs. of  17 variables:\n",
      " $ T   : chr  \"I\" \"D\" \"N\" \"G\" ...\n",
      " $ X2  : int  5 4 7 2 4 4 1 2 11 3 ...\n",
      " $ X8  : int  12 11 11 1 11 2 1 2 15 9 ...\n",
      " $ X3  : int  3 6 6 3 5 5 3 4 13 5 ...\n",
      " $ X5  : int  7 8 6 1 8 4 2 4 9 7 ...\n",
      " $ X1  : int  2 6 3 1 3 4 1 2 7 4 ...\n",
      " $ X8.1: int  10 10 5 8 8 8 8 10 13 8 ...\n",
      " $ X13 : int  5 6 9 6 8 7 2 6 2 7 ...\n",
      " $ X0  : int  5 2 4 6 6 6 2 2 6 3 ...\n",
      " $ X6  : int  4 6 6 6 9 6 2 6 2 8 ...\n",
      " $ X6.1: int  13 10 4 6 5 7 8 12 12 5 ...\n",
      " $ X10 : int  3 3 4 5 6 6 2 4 1 6 ...\n",
      " $ X8.2: int  9 7 10 9 6 6 8 8 9 8 ...\n",
      " $ X0.1: int  2 3 6 1 0 2 1 1 8 2 ...\n",
      " $ X8.3: int  8 7 10 7 8 8 6 6 1 8 ...\n",
      " $ X0.2: int  4 3 2 5 9 7 2 1 1 6 ...\n",
      " $ X8.4: int  10 9 8 10 7 10 7 7 8 7 ...\n"
     ]
    }
   ],
   "source": [
    "str(letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colnames(letters) = c(\"letter\",\n",
    "\"x-box\",\n",
    "\"y-box\",\n",
    "\"width\",\n",
    "\"high\",\n",
    "\"onpix\",\n",
    "\"x-bar\",\n",
    "\"y-bar\",\n",
    "\"x2bar\",\n",
    "\"y2bar\",\n",
    "\"xybar\",\n",
    "\"x2ybr\",\n",
    "\"xy2br\",\n",
    "\"x-ege\",\n",
    "\"xegvy\",\n",
    "\"y-ege\",\n",
    "\"yegvx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color=\"red\"> All the features to be numeric/ Need to specify the random seed </font></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "letters_train = letters[1:16000,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "letters_test = letters[16001:20000,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>letter</th><th scope=col>x-box</th><th scope=col>y-box</th><th scope=col>width</th><th scope=col>high</th><th scope=col>onpix</th><th scope=col>x-bar</th><th scope=col>y-bar</th><th scope=col>x2bar</th><th scope=col>y2bar</th><th scope=col>xybar</th><th scope=col>x2ybr</th><th scope=col>xy2br</th><th scope=col>x-ege</th><th scope=col>xegvy</th><th scope=col>y-ege</th><th scope=col>yegvx</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>16001</th><td>N </td><td>6 </td><td>9 </td><td>8 </td><td>4 </td><td>3 </td><td>8 </td><td>7 </td><td>3 </td><td>4 </td><td>13</td><td>5 </td><td>8 </td><td>6 </td><td>8 </td><td>0 </td><td>8 </td></tr>\n",
       "\t<tr><th scope=row>16002</th><td>V </td><td>6 </td><td>9 </td><td>8 </td><td>8 </td><td>10</td><td>7 </td><td>7 </td><td>5 </td><td>4 </td><td>7 </td><td>6 </td><td>8 </td><td>7 </td><td>9 </td><td>7 </td><td>10</td></tr>\n",
       "\t<tr><th scope=row>16003</th><td>I</td><td>5</td><td>6</td><td>6</td><td>4</td><td>3</td><td>7</td><td>6</td><td>2</td><td>7</td><td>7</td><td>6</td><td>9</td><td>0</td><td>9</td><td>4</td><td>8</td></tr>\n",
       "\t<tr><th scope=row>16004</th><td>N </td><td>5 </td><td>9 </td><td>7 </td><td>6 </td><td>4 </td><td>9 </td><td>7 </td><td>3 </td><td>5 </td><td>10</td><td>4 </td><td>6 </td><td>5 </td><td>8 </td><td>1 </td><td>7 </td></tr>\n",
       "\t<tr><th scope=row>16005</th><td>H </td><td>5 </td><td>8 </td><td>8 </td><td>6 </td><td>6 </td><td>5 </td><td>8 </td><td>3 </td><td>6 </td><td>10</td><td>8 </td><td>8 </td><td>4 </td><td>8 </td><td>4 </td><td>6 </td></tr>\n",
       "\t<tr><th scope=row>16006</th><td>E </td><td>0 </td><td>0 </td><td>1 </td><td>0 </td><td>0 </td><td>5 </td><td>7 </td><td>5 </td><td>6 </td><td>7 </td><td>6 </td><td>12</td><td>0 </td><td>8 </td><td>6 </td><td>10</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllllllllllll}\n",
       "  & letter & x-box & y-box & width & high & onpix & x-bar & y-bar & x2bar & y2bar & xybar & x2ybr & xy2br & x-ege & xegvy & y-ege & yegvx\\\\\n",
       "\\hline\n",
       "\t16001 & N  & 6  & 9  & 8  & 4  & 3  & 8  & 7  & 3  & 4  & 13 & 5  & 8  & 6  & 8  & 0  & 8 \\\\\n",
       "\t16002 & V  & 6  & 9  & 8  & 8  & 10 & 7  & 7  & 5  & 4  & 7  & 6  & 8  & 7  & 9  & 7  & 10\\\\\n",
       "\t16003 & I & 5 & 6 & 6 & 4 & 3 & 7 & 6 & 2 & 7 & 7 & 6 & 9 & 0 & 9 & 4 & 8\\\\\n",
       "\t16004 & N  & 5  & 9  & 7  & 6  & 4  & 9  & 7  & 3  & 5  & 10 & 4  & 6  & 5  & 8  & 1  & 7 \\\\\n",
       "\t16005 & H  & 5  & 8  & 8  & 6  & 6  & 5  & 8  & 3  & 6  & 10 & 8  & 8  & 4  & 8  & 4  & 6 \\\\\n",
       "\t16006 & E  & 0  & 0  & 1  & 0  & 0  & 5  & 7  & 5  & 6  & 7  & 6  & 12 & 0  & 8  & 6  & 10\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "      letter x-box y-box width high onpix x-bar y-bar x2bar y2bar xybar x2ybr\n",
       "16001      N     6     9     8    4     3     8     7     3     4    13     5\n",
       "16002      V     6     9     8    8    10     7     7     5     4     7     6\n",
       "16003      I     5     6     6    4     3     7     6     2     7     7     6\n",
       "16004      N     5     9     7    6     4     9     7     3     5    10     4\n",
       "16005      H     5     8     8    6     6     5     8     3     6    10     8\n",
       "16006      E     0     0     1    0     0     5     7     5     6     7     6\n",
       "      xy2br x-ege xegvy y-ege yegvx\n",
       "16001     8     6     8     0     8\n",
       "16002     8     7     9     7    10\n",
       "16003     9     0     9     4     8\n",
       "16004     6     5     8     1     7\n",
       "16005     8     4     8     4     6\n",
       "16006    12     0     8     6    10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(letters_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(kernlab) # written natively in R and can perform the rescaling automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Setting default kernel parameters  \n"
     ]
    }
   ],
   "source": [
    "set.seed(1234)\n",
    "letterclassifier = ksvm(letter ~ ., data = letters_train, kernel = \"vanilladot\", C= 1)\n",
    "# kernel spedcifies the nonlinear mapping such as \"rbfdot\" (radial basis), \"polydot\"(polynomial), tanhdot(hyperbolic tangent sigmoid), \n",
    "# vanilladot (linear)\n",
    "# C is the penalty parameter the cost of violating the constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Support Vector Machine object of class \"ksvm\" \n",
       "\n",
       "SV type: C-svc  (classification) \n",
       " parameter : cost C = 1 \n",
       "\n",
       "Linear (vanilla) kernel function. \n",
       "\n",
       "Number of Support Vectors : 7039 \n",
       "\n",
       "Objective Function Value : -14.1747 -20.007 -23.5629 -6.2007 -7.5523 -32.7693 -49.9788 -18.1824 -62.111 -32.7284 -16.221 -32.2839 -28.9776 -51.2192 -13.276 -35.6223 -30.8612 -16.5255 -14.681 -32.7472 -30.3216 -7.7959 -11.814 -32.3455 -13.126 -9.2693 -153.165 -52.9678 -76.7743 -119.2073 -165.4435 -54.6247 -41.9818 -67.2686 -25.1959 -27.6368 -26.41 -35.5578 -41.26 -122.1636 -187.9174 -222.0861 -21.4765 -10.3749 -56.3682 -12.2279 -49.4902 -9.3371 -19.2099 -11.1776 -100.2194 -29.14 -238.0507 -77.1985 -8.334 -4.5309 -139.8544 -80.8849 -20.3643 -13.0243 -82.515 -14.5037 -26.7516 -18.5709 -23.9512 -27.3041 -53.273 -11.4773 -5.1202 -13.9501 -4.4981 -3.5754 -8.4912 -40.971 -49.8188 -190.0265 -43.8604 -44.868 -45.258 -13.5555 -17.767 -87.4103 -107.1064 -37.025 -30.713 -112.3208 -32.9635 -27.2966 -35.5832 -17.8585 -5.1394 -43.4089 -7.7841 -16.6797 -58.51 -159.9932 -49.0779 -37.8439 -32.801 -74.5254 -133.3417 -11.164 -5.3575 -12.4375 -30.9902 -141.6928 -54.2953 -179.012 -99.8894 -10.288 -15.1555 -3.7818 -67.612 -7.6958 -88.9304 -47.6447 -94.3718 -70.2735 -71.5066 -21.7856 -12.7654 -7.4383 -23.5023 -13.1052 -239.9699 -30.4194 -25.211 -136.2793 -140.9563 -9.812 -34.4584 -6.304 -60.8422 -66.5785 -27.282 -214.3225 -34.7801 -16.7631 -135.7818 -160.627 -45.2949 -25.1021 -144.9052 -82.2355 -327.7157 -142.0611 -158.8819 -32.2184 -32.8889 -52.9638 -25.4942 -47.9924 -6.8991 -9.7296 -36.4361 -70.3911 -187.7606 -46.9366 -89.8108 -143.4214 -624.3642 -119.2205 -145.4432 -327.7745 -33.3256 -64.0603 -145.4829 -116.5903 -36.2988 -66.3768 -44.8241 -7.509 -217.9246 -12.971 -30.5035 -2.0371 -6.1261 -14.4445 -21.6334 -57.3084 -20.6923 -184.3623 -20.105 -4.1485 -4.5347 -0.8281 -121.4429 -7.9484 -58.5602 -21.4882 -13.5474 -5.6465 -15.6294 -28.9573 -20.5961 -76.7112 -27.0123 -94.7105 -15.1714 -10.0223 -7.6397 -1.5785 -87.6952 -6.2237 -99.3707 -101.0906 -45.6639 -24.0721 -61.7692 -24.1578 -52.2364 -234.326 -39.9757 -48.8561 -34.1458 -20.9665 -11.4524 -123.0291 -6.4901 -5.1868 -8.8018 -9.4612 -21.7736 -24.2361 -123.3978 -31.4396 -88.3897 -30.0912 -13.8194 -9.2701 -3.0825 -87.9616 -6.3842 -13.9679 -65.0712 -105.5232 -13.7404 -13.7627 -50.4226 -2.9331 -8.429 -80.9508 -36.4142 -112.7479 -4.1714 -7.8989 -1.2678 -90.8033 -21.4921 -7.2235 -47.9551 -3.3832 -20.433 -64.6126 -45.5778 -56.1314 -6.1347 -18.6305 -2.3742 -72.2553 -111.188 -106.765 -23.1321 -19.3763 -54.9815 -34.2944 -64.4748 -20.4109 -6.6886 -4.3781 -59.1414 -34.2461 -58.1506 -33.8664 -10.6902 -53.1394 -13.7482 -20.1987 -55.092 -3.8058 -60.0373 -235.484 -12.6837 -11.7408 -17.3059 -9.7171 -65.8491 -17.1047 -42.8136 -53.1058 -25.0432 -15.3018 -44.0747 -16.9584 -62.9777 -5.2037 -5.2966 -86.1709 -3.7209 -6.3449 -1.1265 -122.5773 -23.904 -355.0149 -31.1009 -32.6198 -4.9668 -84.1037 -134.5943 -72.8374 -23.9003 -35.5893 -11.7117 -22.2889 -1.8598 -59.2178 -8.8997 -150.7441 -1.8536 -1.9713 -9.9677 -0.5208 -26.9227 -30.4291 -5.6286 \n",
       "Training error : 0.13025 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "letterclassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "letter_predictions = predict(letterclassifier, letters_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in table(letter_predictions, letters_test$letter): all arguments must have the same length\n",
     "output_type": "error",
     "traceback": [
      "Error in table(letter_predictions, letters_test$letter): all arguments must have the same length\nTraceback:\n",
      "1. table(letter_predictions, letters_test$letter)",
      "2. stop(\"all arguments must have the same length\")"
     ]
    }
   ],
   "source": [
    "table(letter_predictions, letters_test$letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "3999"
      ],
      "text/latex": [
       "3999"
      ],
      "text/markdown": [
       "3999"
      ],
      "text/plain": [
       "[1] 3999"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "length(letter_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>4000</li>\n",
       "\t<li>17</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 4000\n",
       "\\item 17\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 4000\n",
       "2. 17\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 4000   17"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim( letters_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prop.table() # got the percentage "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
