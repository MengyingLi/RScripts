{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align = \"center\"> Feature Selection & Feature Enginnering</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Why feature selection?\n",
    "1) Knowledge discovery: we only want to focus on the important features, Increase interpretability\n",
    "2) Curse of dimensionality: the amount of data that you need grows exponentially to the number of features (in order to fill that space)\n",
    "* NP-hard: search for the subspace of all the combinations of features (2^N)\n",
    "\n",
    "Comparison|Filtering|Wrapping\n",
    "  ------------- | -------------|-------------\n",
    "Methodology| First use some algorithm to find the subset of the features and feed this feature into the ML algorithm (the searching process does not involve the ML algorithm itself) ; They can take advantage of the label but they don't take advantage of the learner |Select a subset of the features and ask the ML algorithm itself how it performs \n",
    "Pros|Faster than wrapping|1) kwrapping built inside what is the learner wants and takes into acount model bias\n",
    "Cons| 1) isolated the features. Some feature might be important in combination of others but it might be removed beforehand2)  There is not any feedback from the ML without referece to the learner|very much slower\n",
    "Example|1) Information gainDecision tree: is a filtering way n the search algorithm and you return the subset of the features and you can use this subset of features in other algorithms. We look at the things got split on. 2) Independent features not the linear combination of existing features 3) Entropy (does not depend on the label) but Information gain depends on the label |1) Hill climbing 2)Randomization optimization 3) Forward and backward search:(still link to the learning algorithm) [still hill climbing]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FSelector Package for R offers algorithms for filtering attributes (e.g. cfs, chi-squared, information gain, linear correlation) and algorithms for wrapping classifiers and search attribute subset space (e.g. best-first search, back-ward search, forward search, hill climbing search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(https://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Dimensionality_Reduction/Feature_Selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
