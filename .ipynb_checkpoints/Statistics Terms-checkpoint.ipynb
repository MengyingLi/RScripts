{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align = \"center\"> Statistical Terms </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Survivial Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1 - cumulative distribution (time to event data) P(T>t) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Smoothing of the curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Help remove the systematical variance of other variables and make sure the result is different is only due to the experiment manipulation.\n",
    "* Systematic variation: This variation is due to the experimenter doing something to all of the participants in one condition but not in the other condition.\n",
    "* Unsystematic variation: This variation results from random factors that exist between the experimental conditions (natural dif erences in ability, the time of day, etc.).\n",
    "* test statistics are usually the amount of systematic variance divided by the amount of unsystematic variance/ the model compared against the error in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skewness & Kurtosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Kurtosis: this measures the degree to which scores cluster in the tails of a frequency distribution. Adistribution with positive kurtosis (leptokurtic, kurtosis > 0) has too many scores in the tails and is too peaked, whereas a distribution with negative kurtosis (platykurtic, kurtosis < 0) has too few scores in the tails and is quite flat.\n",
    "** A distribution with positive kurtosis hasmany scores in the tails (a so-called heavy-tailed distribution) and is pointy. This is known as a leptokurtic distribution (瘦子）. In contrast, a distribution with negative kurtosis is is relatively thin in\n",
    "the tails (has light tails) and tends to be flatter than normal (胖子）. This distribution is called\n",
    "platykurtic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Degree of freedom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In statistical terms the degrees of freedom relate to the number of observations that are free to vary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As well as telling us about the accuracy of the mean as a model of our data set, the variance and standard deviation also tell us about the shape of the distribution of scores。 The larger the fatter but it did not imply the kurtosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The standard error is the standard deviation of sample means. As such, it is a measure of how representativesample is likely to be of the population. A large standard error (relative to the sample mean) means that there is a lot of variability between the means of different samples and so the sample we have might not be representative of the population. Asmall standard error indicates that most sample means are similar to the population mean and so our sample is likely to be an accurate reflection of the population.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confidence Interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* A confidence interval for the mean is a range of scores constructed such that the population mean will fall within this range in 95% of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* To reiterate, when an experimental manipulation is successful, we expect to find that our samples have come from different populations. If the manipulation is unsuccessful, then we expect to find that the samples came from the same population (e.g., the sample means should be fairly similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* All that a non-significant result tells us is that the effect is not big enough to be anything other than a chance finding – it doesn’t tell us that the effect is zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effect Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Effect sizes are an invaluable way to express the importance of a research finding. The effect size in a population is intrinsically linked to three other statistical properties: (1) the sample size on which the sample effect size is based; (2) the probability level at which we will accept an effect as being statistically significant (the α-level); and (3) the ability of a test to detect an effect of that size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simpson Paradox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Simpson’s paradox arises whenever an apparent trend in data, caused by a confounding variable, can be eliminated or reversed by splitting the data into natural groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assumptions for Parametric Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Normally distributed (For example, the sample mean needs to be normally distributed)\n",
    "* Homogeneity of variance. This assumption means that the variances should be the same throughout the data. In designs in which you test several groups of participants this assumption means that each of these samples comes from populations with the same variance. In correlational designs, this assumption means that the variance of one variable should be stable at all levels of the other variable.\n",
    "* Independence\n",
    "* Should measurea at least at interval level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\n",
    "* Remove the case: This entails deleting the data from the person who\n",
    "contributed the outlier. However, this should be done only if you have good\n",
    "reason to believe that this case is not from the population that you intended\n",
    "to sample. For example, if you were investigating factors that affected how\n",
    "much cats purr and one cat didn’t purr at all, this would likely be an outlier\n",
    "(all cats purr). Upon inspection, if you discovered that this cat was actually\n",
    "a dog wearing a cat costume (hence why it didn’t purr), then you’d have\n",
    "grounds to exclude this case because it comes from a different population\n",
    "(dogs who like to dress as cats) than your target population (cats).\n",
    "\n",
    "2\n",
    "* Transform the data: Outliers tend to skew the distribution and, as we will\n",
    "see in the next section, this skew (and, therefore, the impact of the outliers)\n",
    "can sometimes be reduced by applying transformations to the data.\n",
    "\n",
    "a. Log transformation: Positive Skew and unequal variance (if you have negative numbers not plus one but plus the number which will convert the smallest to be 0)\n",
    "\n",
    "b. Square root similar as log\n",
    "\n",
    "c. Reciprocal transformation (1/X) similar as log\n",
    "\n",
    "d. Reverse score transformation: subtract each score from the highest score obtained: Negative Skewed\n",
    "\n",
    "e. It is important to test the normality assumption. If the data are in fact clearly not normal, the Box-Cox normality plot can often be used to find a transformation that will approximately normalize the data.\n",
    "\n",
    "However, transformation will skew the test you want to conduct\n",
    "\n",
    "3\n",
    "* Change the score: If transformation fails, then you can consider replacing\n",
    "the score. This on the face of it may seem like cheating (you’re changing the\n",
    "data from what was actually corrected); however, if the score you’re\n",
    "changing is very unrepresentative and biases your statistical model anyway\n",
    "then changing the score is the lesser of two evils! There are several options\n",
    "for how to change the score:\n",
    "\n",
    "a. The next highest score plus one: Change the score to be one unit above\n",
    "the next highest score in the data set.\n",
    "    \n",
    "b.Convert back from a z-score: A z-score of 3.29 constitutes an outlier\n",
    ", so we can calculate what score would give rise to a z-score of 3.29 (or perhaps 3) by rearranging the z-score\n",
    "equation in section 1.7.4, which gives us . All this means is that we calculate the mean and standard deviation (s) of the data;\n",
    "we know that z is 3 (or 3.29 if you want to be exact) so we just add three times the standard deviation to the mean, and replace our outliers with\n",
    "that score.\n",
    "\n",
    "c. The mean plus two standard deviations: A variation on the above\n",
    "method is to use the mean plus two times the standard deviation (rather\n",
    "than three times the standard deviation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If we are interested in whether two variables are related, then we\n",
    "are interested in whether changes in one variable are met with similar changes in the other\n",
    "variable. Therefore, when one variable deviates from its mean we would expect the other variable\n",
    "to deviate from its mean in a similar way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* By standardizing the covariance we end up with a value that has to lie between −1 and +1 (if\n",
    "you find a correlation coefficient less than −1 or more than +1 and the transformation of this correlation has a normal distribution and we can infer it with z-score\n",
    "* Two types of correlation: bivariate and partial. \n",
    "* We can measure the relationship between two variables using correlation coefficients.\n",
    "These coefficients lie between -1 and +1.\n",
    "* Pearson’s correlation coefficient, r, is a parametric statistic and requires interval data for both\n",
    "variables. To test its significance we assume normality too.\n",
    "* Spearman’s correlation coefficient, rs, is a nonparametric statistic and requires only ordinal data for\n",
    "both variables.\n",
    "* Kendall’s correlation coefficient, τ, is like Spearman’s rs but probably better for small samples.\n",
    "* The point-biserial correlation coefficient, rpb, quantifies the relationship between a continuous variable\n",
    "and a variable that is a discrete dichotomy (e.g., there is no continuum underlying the two categories,\n",
    "such as dead or alive).\n",
    "* The biserial correlation coefficient, rb, quantifies the relationship between a continuous variable and a\n",
    "variable that is a continuous dichotomy (e.g., there is a continuum underlying the two categories, such as\n",
    "passing or failing an exam)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R-squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a measure of the amount of variability in one variable that\n",
    "is shared by the other. (Pearson's R)\n",
    "* If use the Spearman's Rho, we should interpret R-squared as it is the proportion of\n",
    "variance in the ranks that two variables share"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partial & Semi-partial correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A partial correlation quantifies the relationship between two variables while controlling for the effects\n",
    "of a third variable on both variables in the original correlation.\n",
    "A semi-partial correlation quantifies the relationship between two variables while controlling for the\n",
    "effects of a third variable on only one of the variables in the original correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### AIC/BIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine we add a variable to the model; usually this would increase R2, and hence SSE would\n",
    "be reduced. But imagine that this variable does not change the fit of the model at all. What will\n",
    "happen to the AIC? Well, the first part will be the same: n and SSE are unchanged. What will\n",
    "change is k: it will be higher, by one (because we have added a variable). Hence, when we add\n",
    "this variable to the model, the AIC will be higher by 2. A larger value of the AIC indicates worse\n",
    "fit, corrected for the number of variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "An outlier is a case that differs substantially from the main trend of the data. Detecting outliers by looking at the error in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change continuous variable to categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Might confuse the similarity because of the observations fall on the opposite side of the cutoff\n",
    "* Effect size get smaller (the correlation)\n",
    "* an increased chance of finding spurious effects\n",
    "* One of the rare situations in which dichotomizing a continuous\n",
    "variable is justified, according to MacCallum et al., is when there is a clear theoretical rationale for distinct\n",
    "categories of people based on a meaningful break point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-statistics explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Two samples of data are collected and the sample means calculated. These means might\n",
    "differ by either a little or a lot.\n",
    "* If the samples come from the same population, then we expect their means to be roughly equal\n",
    ". Although it is possible for their means to differ by chance alone, we\n",
    "would expect large differences between sample means to occur very infrequently. Under the\n",
    "null hypothesis we assume that the experimental manipulation has no effect on the\n",
    "participants: therefore, we expect the sample means to be very similar.\n",
    "* We compare the difference between the sample means that we collected to the difference\n",
    "between the sample means that we would expect to obtain if there were no effect (i.e., if the\n",
    "null hypothesis were true). We use the standard error (see section 2.5.1) as a gauge of the\n",
    "variability between sample means. If the standard error is small, then we expect most\n",
    "samples to have very similar means. When the standard error is large, large differences in\n",
    "sample means are more likely. If the difference between the samples we have collected is\n",
    "larger than we would expect based on the standard error then we can assume one of two\n",
    "things:\n",
    "    * There is no effect and sample means in our population fluctuate a lot and we have, by\n",
    "chance, collected two samples that are atypical of the population from which they came.\n",
    "    * The two samples come from different populations but are typical of their respective parent\n",
    "population. In this scenario, the difference between samples represents a genuine difference\n",
    "between the samples (and so the null hypothesis is incorrect).\n",
    "* As the observed difference between the sample means gets larger, the more confident we\n",
    "become that the second explanation is correct (i.e., that the null hypothesis should be\n",
    "rejected). If the null hypothesis is incorrect, then we gain confidence that the two sample\n",
    "means differ because of the different experimental manipulation imposed on each sample.  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><font color = \"red\"> if the independent t-test the sample sizes are different, use pooled variance (poolvar<-(((n1-1)*sd1^2)+((n2-1)*sd2^2))/df)instead of (s1^2/n1 - s2^2/n2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### ANOVA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Whether the mean across more than 2 groups are the same or different\n",
    "* Turning to violations of the assumption of homogeneity of variance, ANOVA is fairly robust in\n",
    "terms of the error rate when sample sizes are equal. However, when sample sizes are unequal,\n",
    "ANOVA is not robust to violations of homogeneity of variance\n",
    "* Control Type I error based on Befferoni or BH will sacrifice the power or we should use FDR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
